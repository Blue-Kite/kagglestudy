{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-10T02:20:01.049574Z","iopub.execute_input":"2021-11-10T02:20:01.050052Z","iopub.status.idle":"2021-11-10T02:20:01.061147Z","shell.execute_reply.started":"2021-11-10T02:20:01.050015Z","shell.execute_reply":"2021-11-10T02:20:01.060139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:01.07192Z","iopub.execute_input":"2021-11-10T02:20:01.072344Z","iopub.status.idle":"2021-11-10T02:20:01.081089Z","shell.execute_reply.started":"2021-11-10T02:20:01.072314Z","shell.execute_reply":"2021-11-10T02:20:01.080299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/costa-rican-household-poverty-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:01.082902Z","iopub.execute_input":"2021-11-10T02:20:01.083572Z","iopub.status.idle":"2021-11-10T02:20:01.416771Z","shell.execute_reply.started":"2021-11-10T02:20:01.083531Z","shell.execute_reply":"2021-11-10T02:20:01.416003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:01.418523Z","iopub.execute_input":"2021-11-10T02:20:01.418867Z","iopub.status.idle":"2021-11-10T02:20:01.444475Z","shell.execute_reply.started":"2021-11-10T02:20:01.418828Z","shell.execute_reply":"2021-11-10T02:20:01.4436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n                                                                             figsize = (8, 6),\n                                                                            edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values')\nplt.ylabel('Count')\nplt.title('Count of Unique Values in Integer Columns');","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:01.44614Z","iopub.execute_input":"2021-11-10T02:20:01.446433Z","iopub.status.idle":"2021-11-10T02:20:01.740323Z","shell.execute_reply.started":"2021-11-10T02:20:01.446395Z","shell.execute_reply":"2021-11-10T02:20:01.739607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    for poverty_level, color in colors.items():\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:01.742596Z","iopub.execute_input":"2021-11-10T02:20:01.744408Z","iopub.status.idle":"2021-11-10T02:20:03.836839Z","shell.execute_reply.started":"2021-11-10T02:20:01.744363Z","shell.execute_reply":"2021-11-10T02:20:03.836103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.select_dtypes('object').head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:03.838229Z","iopub.execute_input":"2021-11-10T02:20:03.838737Z","iopub.status.idle":"2021-11-10T02:20:03.854182Z","shell.execute_reply.started":"2021-11-10T02:20:03.838689Z","shell.execute_reply":"2021-11-10T02:20:03.853512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\nfor df in [train, test]:  \n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n\ntrain[['dependency', 'edjefa', 'edjefe']].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:03.855687Z","iopub.execute_input":"2021-11-10T02:20:03.85614Z","iopub.status.idle":"2021-11-10T02:20:03.91694Z","shell.execute_reply.started":"2021-11-10T02:20:03.856101Z","shell.execute_reply":"2021-11-10T02:20:03.915897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 12))\n\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i + 1)\n    for poverty_level, color in colors.items():\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        \n    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:03.918392Z","iopub.execute_input":"2021-11-10T02:20:03.918661Z","iopub.status.idle":"2021-11-10T02:20:04.76864Z","shell.execute_reply.started":"2021-11-10T02:20:03.918625Z","shell.execute_reply":"2021-11-10T02:20:04.766894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Target'] = np.nan\ndata = train.append(test, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:04.770056Z","iopub.execute_input":"2021-11-10T02:20:04.770563Z","iopub.status.idle":"2021-11-10T02:20:04.801207Z","shell.execute_reply.started":"2021-11-10T02:20:04.770523Z","shell.execute_reply":"2021-11-10T02:20:04.800499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads = data.loc[data['parentesco1'] == 1].copy()\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]\n\n\nlabel_counts = train_labels['Target'].value_counts().sort_index()\nlabel_counts.plot.bar(figsize = (8, 6), color = colors.values(), edgecolor = 'k', linewidth = 2)\n\n\nplt.xlabel('Poverty Level'); plt.ylabel('Count'); \nplt.xticks([x - 1 for x in poverty_mapping.keys()], \n           list(poverty_mapping.values()), rotation = 60)\nplt.title('Poverty Level Breakdown');\n\nlabel_counts","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:04.802759Z","iopub.execute_input":"2021-11-10T02:20:04.803311Z","iopub.status.idle":"2021-11-10T02:20:05.079515Z","shell.execute_reply.started":"2021-11-10T02:20:04.80326Z","shell.execute_reply":"2021-11-10T02:20:05.078797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.08293Z","iopub.execute_input":"2021-11-10T02:20:05.083142Z","iopub.status.idle":"2021-11-10T02:20:05.352661Z","shell.execute_reply.started":"2021-11-10T02:20:05.083116Z","shell.execute_reply":"2021-11-10T02:20:05.351852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.353788Z","iopub.execute_input":"2021-11-10T02:20:05.354326Z","iopub.status.idle":"2021-11-10T02:20:05.368735Z","shell.execute_reply.started":"2021-11-10T02:20:05.354286Z","shell.execute_reply":"2021-11-10T02:20:05.367583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"households_leader = train.groupby('idhogar')['parentesco1'].sum()\nhouseholds_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.370202Z","iopub.execute_input":"2021-11-10T02:20:05.370929Z","iopub.status.idle":"2021-11-10T02:20:05.385872Z","shell.execute_reply.started":"2021-11-10T02:20:05.37089Z","shell.execute_reply":"2021-11-10T02:20:05.384997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nprint('{} Households with no head have different labels.'.format(sum(households_no_head_equal == False)))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.387362Z","iopub.execute_input":"2021-11-10T02:20:05.387631Z","iopub.status.idle":"2021-11-10T02:20:05.396749Z","shell.execute_reply.started":"2021-11-10T02:20:05.387594Z","shell.execute_reply":"2021-11-10T02:20:05.395822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for household in not_equal.index:\n    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n    train.loc[train['idhogar'] == household, 'Target'] = true_target\n    \n    \n\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.398334Z","iopub.execute_input":"2021-11-10T02:20:05.398997Z","iopub.status.idle":"2021-11-10T02:20:05.918136Z","shell.execute_reply.started":"2021-11-10T02:20:05.398953Z","shell.execute_reply":"2021-11-10T02:20:05.917296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\nmissing['percent'] = missing['total'] / len(data)\n\nmissing.sort_values('percent', ascending = False).head(10).drop('Target')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.919681Z","iopub.execute_input":"2021-11-10T02:20:05.919977Z","iopub.status.idle":"2021-11-10T02:20:05.950313Z","shell.execute_reply.started":"2021-11-10T02:20:05.919939Z","shell.execute_reply":"2021-11-10T02:20:05.949492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_value_counts(df, col, heads_only = False):\n    if heads_only:\n        df = df.loc[df['parentesco1'] == 1].copy()\n        \n    plt.figure(figsize = (8, 6))\n    df[col].value_counts().sort_index().plot.bar(color = 'blue',\n                                                 edgecolor = 'k',\n                                                 linewidth = 2)\n    plt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.951649Z","iopub.execute_input":"2021-11-10T02:20:05.951952Z","iopub.status.idle":"2021-11-10T02:20:05.960297Z","shell.execute_reply.started":"2021-11-10T02:20:05.951911Z","shell.execute_reply":"2021-11-10T02:20:05.959262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_value_counts(heads, 'v18q1')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:05.962087Z","iopub.execute_input":"2021-11-10T02:20:05.962377Z","iopub.status.idle":"2021-11-10T02:20:06.175289Z","shell.execute_reply.started":"2021-11-10T02:20:05.962341Z","shell.execute_reply":"2021-11-10T02:20:06.174565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.176754Z","iopub.execute_input":"2021-11-10T02:20:06.177263Z","iopub.status.idle":"2021-11-10T02:20:06.186651Z","shell.execute_reply.started":"2021-11-10T02:20:06.177201Z","shell.execute_reply":"2021-11-10T02:20:06.185771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['v18q1'] = data['v18q1'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.188508Z","iopub.execute_input":"2021-11-10T02:20:06.188829Z","iopub.status.idle":"2021-11-10T02:20:06.194498Z","shell.execute_reply.started":"2021-11-10T02:20:06.188792Z","shell.execute_reply":"2021-11-10T02:20:06.193587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"own_variables = [x for x in data if x.startswith('tipo')]\ndata.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),color = 'green', edgecolor = 'k', linewidth = 2);\nplt.xticks([0, 1, 2, 3, 4],\n           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n          rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.195795Z","iopub.execute_input":"2021-11-10T02:20:06.196102Z","iopub.status.idle":"2021-11-10T02:20:06.591804Z","shell.execute_reply.started":"2021-11-10T02:20:06.196066Z","shell.execute_reply":"2021-11-10T02:20:06.591133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\ndata['v2a1-missing'] = data['v2a1'].isnull()\ndata['v2a1-missing'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.59306Z","iopub.execute_input":"2021-11-10T02:20:06.593744Z","iopub.status.idle":"2021-11-10T02:20:06.60791Z","shell.execute_reply.started":"2021-11-10T02:20:06.593701Z","shell.execute_reply":"2021-11-10T02:20:06.606972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[data['rez_esc'].notnull()]['age'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.609197Z","iopub.execute_input":"2021-11-10T02:20:06.609488Z","iopub.status.idle":"2021-11-10T02:20:06.626221Z","shell.execute_reply.started":"2021-11-10T02:20:06.609453Z","shell.execute_reply":"2021-11-10T02:20:06.625444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[data['rez_esc'].isnull()]['age'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.627637Z","iopub.execute_input":"2021-11-10T02:20:06.628056Z","iopub.status.idle":"2021-11-10T02:20:06.649001Z","shell.execute_reply.started":"2021-11-10T02:20:06.628017Z","shell.execute_reply":"2021-11-10T02:20:06.648268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.650223Z","iopub.execute_input":"2021-11-10T02:20:06.650673Z","iopub.status.idle":"2021-11-10T02:20:06.661192Z","shell.execute_reply.started":"2021-11-10T02:20:06.65063Z","shell.execute_reply":"2021-11-10T02:20:06.660473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.662531Z","iopub.execute_input":"2021-11-10T02:20:06.662907Z","iopub.status.idle":"2021-11-10T02:20:06.668791Z","shell.execute_reply.started":"2021-11-10T02:20:06.662869Z","shell.execute_reply":"2021-11-10T02:20:06.667998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"feature engineer","metadata":{}},{"cell_type":"code","source":"id_ = ['Id', 'idhogar', 'Target']\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']\n\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']\n\nsqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.670054Z","iopub.execute_input":"2021-11-10T02:20:06.670654Z","iopub.status.idle":"2021-11-10T02:20:06.68291Z","shell.execute_reply.started":"2021-11-10T02:20:06.670599Z","shell.execute_reply":"2021-11-10T02:20:06.682071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n\nfrom collections import Counter\n\nprint('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\nprint('We covered every variable: ', len(x) == data.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.684199Z","iopub.execute_input":"2021-11-10T02:20:06.684605Z","iopub.status.idle":"2021-11-10T02:20:06.696318Z","shell.execute_reply.started":"2021-11-10T02:20:06.68457Z","shell.execute_reply":"2021-11-10T02:20:06.695424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot('age', 'SQBage', data = data, fit_reg=False);\nplt.title('Squared Age versus Age');","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:06.703476Z","iopub.execute_input":"2021-11-10T02:20:06.703969Z","iopub.status.idle":"2021-11-10T02:20:07.125986Z","shell.execute_reply.started":"2021-11-10T02:20:06.703925Z","shell.execute_reply":"2021-11-10T02:20:07.125287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.drop(columns = sqr_)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.127287Z","iopub.execute_input":"2021-11-10T02:20:07.127639Z","iopub.status.idle":"2021-11-10T02:20:07.144821Z","shell.execute_reply.started":"2021-11-10T02:20:07.127597Z","shell.execute_reply":"2021-11-10T02:20:07.1441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads = data.loc[data['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.146696Z","iopub.execute_input":"2021-11-10T02:20:07.147292Z","iopub.status.idle":"2021-11-10T02:20:07.167165Z","shell.execute_reply.started":"2021-11-10T02:20:07.147161Z","shell.execute_reply":"2021-11-10T02:20:07.166531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = heads.corr()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.168422Z","iopub.execute_input":"2021-11-10T02:20:07.168671Z","iopub.status.idle":"2021-11-10T02:20:07.430829Z","shell.execute_reply.started":"2021-11-10T02:20:07.168638Z","shell.execute_reply":"2021-11-10T02:20:07.429889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.432043Z","iopub.execute_input":"2021-11-10T02:20:07.433426Z","iopub.status.idle":"2021-11-10T02:20:07.447666Z","shell.execute_reply.started":"2021-11-10T02:20:07.433386Z","shell.execute_reply":"2021-11-10T02:20:07.446825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n            annot=True, cmap = plt.cm.autumn_r, fmt='.3f')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.449147Z","iopub.execute_input":"2021-11-10T02:20:07.449672Z","iopub.status.idle":"2021-11-10T02:20:07.933795Z","shell.execute_reply.started":"2021-11-10T02:20:07.449632Z","shell.execute_reply":"2021-11-10T02:20:07.933063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.937819Z","iopub.execute_input":"2021-11-10T02:20:07.939767Z","iopub.status.idle":"2021-11-10T02:20:07.949505Z","shell.execute_reply.started":"2021-11-10T02:20:07.939723Z","shell.execute_reply":"2021-11-10T02:20:07.948696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot('tamviv', 'hhsize', data, fit_reg=False, size = 8);\nplt.title('Household size vs number of persons living in the household');","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:07.953833Z","iopub.execute_input":"2021-11-10T02:20:07.955963Z","iopub.status.idle":"2021-11-10T02:20:08.488822Z","shell.execute_reply.started":"2021-11-10T02:20:07.955921Z","shell.execute_reply":"2021-11-10T02:20:08.488124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:08.490175Z","iopub.execute_input":"2021-11-10T02:20:08.490444Z","iopub.status.idle":"2021-11-10T02:20:08.49586Z","shell.execute_reply.started":"2021-11-10T02:20:08.490412Z","shell.execute_reply":"2021-11-10T02:20:08.495097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9, corr_matrix['coopele'].abs() > 0.9]","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:08.497206Z","iopub.execute_input":"2021-11-10T02:20:08.49766Z","iopub.status.idle":"2021-11-10T02:20:08.514022Z","shell.execute_reply.started":"2021-11-10T02:20:08.497623Z","shell.execute_reply":"2021-11-10T02:20:08.513354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elec = []\n\nfor i, row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n\nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:08.515387Z","iopub.execute_input":"2021-11-10T02:20:08.515673Z","iopub.status.idle":"2021-11-10T02:20:09.09497Z","shell.execute_reply.started":"2021-11-10T02:20:08.515636Z","shell.execute_reply":"2021-11-10T02:20:09.094151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads = heads.drop(columns = 'area2')\nheads.groupby('area1')['Target'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.096231Z","iopub.execute_input":"2021-11-10T02:20:09.098341Z","iopub.status.idle":"2021-11-10T02:20:09.115005Z","shell.execute_reply.started":"2021-11-10T02:20:09.098312Z","shell.execute_reply":"2021-11-10T02:20:09.114146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.116481Z","iopub.execute_input":"2021-11-10T02:20:09.116776Z","iopub.status.idle":"2021-11-10T02:20:09.123342Z","shell.execute_reply.started":"2021-11-10T02:20:09.116734Z","shell.execute_reply":"2021-11-10T02:20:09.122427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]), axis = 1)\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.125048Z","iopub.execute_input":"2021-11-10T02:20:09.125585Z","iopub.status.idle":"2021-11-10T02:20:09.143905Z","shell.execute_reply.started":"2021-11-10T02:20:09.125538Z","shell.execute_reply":"2021-11-10T02:20:09.143109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.145368Z","iopub.execute_input":"2021-11-10T02:20:09.145686Z","iopub.status.idle":"2021-11-10T02:20:09.151149Z","shell.execute_reply.started":"2021-11-10T02:20:09.145643Z","shell.execute_reply":"2021-11-10T02:20:09.150325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = pd.DataFrame(heads.groupby(['walls+roof+floor'])['Target'].value_counts(normalize = True)).rename(columns = {'Target': 'Normalized Count'}).reset_index()\ncounts.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.152751Z","iopub.execute_input":"2021-11-10T02:20:09.153315Z","iopub.status.idle":"2021-11-10T02:20:09.175023Z","shell.execute_reply.started":"2021-11-10T02:20:09.153269Z","shell.execute_reply":"2021-11-10T02:20:09.17423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['warning'] = 1 * (heads['sanitario1'] + (heads['elec'] == 0) + heads['pisonotiene'] + heads['abastaguano'] + (heads['cielorazo'] == 0))","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.176432Z","iopub.execute_input":"2021-11-10T02:20:09.176725Z","iopub.status.idle":"2021-11-10T02:20:09.184099Z","shell.execute_reply.started":"2021-11-10T02:20:09.176687Z","shell.execute_reply":"2021-11-10T02:20:09.183146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['bonus'] = 1 * (heads['refrig'] + heads['computer'] + (heads['v18q1'] > 0) + heads['television'])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.185557Z","iopub.execute_input":"2021-11-10T02:20:09.185917Z","iopub.status.idle":"2021-11-10T02:20:09.196619Z","shell.execute_reply.started":"2021-11-10T02:20:09.185877Z","shell.execute_reply":"2021-11-10T02:20:09.19571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.198169Z","iopub.execute_input":"2021-11-10T02:20:09.198499Z","iopub.status.idle":"2021-11-10T02:20:09.208953Z","shell.execute_reply.started":"2021-11-10T02:20:09.198463Z","shell.execute_reply":"2021-11-10T02:20:09.208122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import spearmanr","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.210528Z","iopub.execute_input":"2021-11-10T02:20:09.210734Z","iopub.status.idle":"2021-11-10T02:20:09.21658Z","shell.execute_reply.started":"2021-11-10T02:20:09.210703Z","shell.execute_reply":"2021-11-10T02:20:09.215574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_corrs(x, y):\n    \n    spr = spearmanr(x, y).correlation\n    pcr = np.corrcoef(x, y)[0, 1]\n    \n    data = pd.DataFrame({'x': x, 'y': y})\n    plt.figure( figsize = (6, 4))\n    sns.regplot('x', 'y', data = data, fit_reg = False);\n    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}');","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.218337Z","iopub.execute_input":"2021-11-10T02:20:09.21889Z","iopub.status.idle":"2021-11-10T02:20:09.227012Z","shell.execute_reply.started":"2021-11-10T02:20:09.218849Z","shell.execute_reply":"2021-11-10T02:20:09.226154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array(range(100))\ny = x ** 2\n\nplot_corrs(x, y)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.229798Z","iopub.execute_input":"2021-11-10T02:20:09.230051Z","iopub.status.idle":"2021-11-10T02:20:09.459785Z","shell.execute_reply.started":"2021-11-10T02:20:09.230025Z","shell.execute_reply":"2021-11-10T02:20:09.459089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_heads = heads.loc[heads['Target'].notnull(), :].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target': 'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns = {'index': 'feature'})\n\nprint('Most negatively correlated variables:')\nprint(pcorrs.head())\n\nprint('\\nMost positively correlated variables:')\nprint(pcorrs.dropna().tail())","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.461032Z","iopub.execute_input":"2021-11-10T02:20:09.462432Z","iopub.status.idle":"2021-11-10T02:20:09.557405Z","shell.execute_reply.started":"2021-11-10T02:20:09.462388Z","shell.execute_reply":"2021-11-10T02:20:09.55659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\nfeats = []\nscorr = []\npvalues = []\n\nfor c in heads:\n    if heads[c].dtype != 'object':\n        feats.append(c)\n        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n\nscorrs = pd.DataFrame({'feature': feats, 'scorr': scorr, 'pvalue': pvalues}).sort_values('scorr')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.558621Z","iopub.execute_input":"2021-11-10T02:20:09.55934Z","iopub.status.idle":"2021-11-10T02:20:09.776052Z","shell.execute_reply.started":"2021-11-10T02:20:09.559299Z","shell.execute_reply":"2021-11-10T02:20:09.775261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Most negative Spearman correlations:')\nprint(scorrs.head())\nprint('\\nMost positive Spearman correlations:')\nprint(scorrs.dropna().tail())","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.777319Z","iopub.execute_input":"2021-11-10T02:20:09.778199Z","iopub.status.idle":"2021-11-10T02:20:09.794216Z","shell.execute_reply.started":"2021-11-10T02:20:09.778155Z","shell.execute_reply":"2021-11-10T02:20:09.793325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs = pcorrs.merge(scorrs, on = 'feature')\ncorrs['diff'] = corrs['pcorr'] - corrs['scorr']\n\ncorrs.sort_values('diff').head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.796019Z","iopub.execute_input":"2021-11-10T02:20:09.796742Z","iopub.status.idle":"2021-11-10T02:20:09.815005Z","shell.execute_reply.started":"2021-11-10T02:20:09.796712Z","shell.execute_reply":"2021-11-10T02:20:09.814149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs.sort_values('diff').dropna().tail()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.816632Z","iopub.execute_input":"2021-11-10T02:20:09.816939Z","iopub.status.idle":"2021-11-10T02:20:09.834904Z","shell.execute_reply.started":"2021-11-10T02:20:09.816901Z","shell.execute_reply":"2021-11-10T02:20:09.834109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot('dependency', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Dependency');","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:09.83623Z","iopub.execute_input":"2021-11-10T02:20:09.836483Z","iopub.status.idle":"2021-11-10T02:20:10.39083Z","shell.execute_reply.started":"2021-11-10T02:20:09.83645Z","shell.execute_reply":"2021-11-10T02:20:10.390072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variables = ['Target', 'dependency', 'warning', 'walls+roof+floor', 'meaneduc', 'floor', 'r4m1', 'overcrowding']\n\ncorr_mat = train_heads[variables].corr().round(2)\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n            cmap = plt.cm.RdYlGn_r, annot = True);","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:10.392308Z","iopub.execute_input":"2021-11-10T02:20:10.392809Z","iopub.status.idle":"2021-11-10T02:20:11.06611Z","shell.execute_reply.started":"2021-11-10T02:20:10.392768Z","shell.execute_reply":"2021-11-10T02:20:11.065396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"household_feats = list(heads.columns)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:11.067383Z","iopub.execute_input":"2021-11-10T02:20:11.067764Z","iopub.status.idle":"2021-11-10T02:20:11.07214Z","shell.execute_reply.started":"2021-11-10T02:20:11.067728Z","shell.execute_reply":"2021-11-10T02:20:11.071148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:11.073652Z","iopub.execute_input":"2021-11-10T02:20:11.074095Z","iopub.status.idle":"2021-11-10T02:20:11.089819Z","shell.execute_reply.started":"2021-11-10T02:20:11.074056Z","shell.execute_reply":"2021-11-10T02:20:11.088903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = ind.corr()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:11.09098Z","iopub.execute_input":"2021-11-10T02:20:11.091662Z","iopub.status.idle":"2021-11-10T02:20:11.228499Z","shell.execute_reply.started":"2021-11-10T02:20:11.091619Z","shell.execute_reply":"2021-11-10T02:20:11.227512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind = ind.drop(columns = 'male')","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:11.229932Z","iopub.execute_input":"2021-11-10T02:20:11.230317Z","iopub.status.idle":"2021-11-10T02:20:11.240304Z","shell.execute_reply.started":"2021-11-10T02:20:11.230276Z","shell.execute_reply":"2021-11-10T02:20:11.239411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind[[c for c in ind if c.startswith('instl')]].head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:11.241536Z","iopub.execute_input":"2021-11-10T02:20:11.241893Z","iopub.status.idle":"2021-11-10T02:20:11.260745Z","shell.execute_reply.started":"2021-11-10T02:20:11.241851Z","shell.execute_reply":"2021-11-10T02:20:11.259921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\n\nplot_categoricals('inst', 'Target', ind, annotate = False);","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:20:11.262662Z","iopub.execute_input":"2021-11-10T02:20:11.262947Z","iopub.status.idle":"2021-11-10T02:20:11.286305Z","shell.execute_reply.started":"2021-11-10T02:20:11.262908Z","shell.execute_reply":"2021-11-10T02:20:11.285284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:21:28.089325Z","iopub.execute_input":"2021-11-10T02:21:28.090096Z","iopub.status.idle":"2021-11-10T02:21:28.394833Z","shell.execute_reply.started":"2021-11-10T02:21:28.090047Z","shell.execute_reply":"2021-11-10T02:21:28.394174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind['escolari/age'] = ind['escolari'] / ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari/age', data = ind);","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:21:34.212337Z","iopub.execute_input":"2021-11-10T02:21:34.213135Z","iopub.status.idle":"2021-11-10T02:21:34.521339Z","shell.execute_reply.started":"2021-11-10T02:21:34.213087Z","shell.execute_reply":"2021-11-10T02:21:34.520636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind['inst/age'] = ind['inst'] / ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:21:41.550117Z","iopub.execute_input":"2021-11-10T02:21:41.550797Z","iopub.status.idle":"2021-11-10T02:21:41.565109Z","shell.execute_reply.started":"2021-11-10T02:21:41.55076Z","shell.execute_reply":"2021-11-10T02:21:41.564294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define custom function\nrange_ = lambda x: x.max() - x.min()\nrange_.__name__ = 'range_'\n\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:21:47.093073Z","iopub.execute_input":"2021-11-10T02:21:47.093339Z","iopub.status.idle":"2021-11-10T02:21:48.723376Z","shell.execute_reply.started":"2021-11-10T02:21:47.093308Z","shell.execute_reply":"2021-11-10T02:21:48.722133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_col = []\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:21:58.341372Z","iopub.execute_input":"2021-11-10T02:21:58.341639Z","iopub.status.idle":"2021-11-10T02:21:58.358567Z","shell.execute_reply.started":"2021-11-10T02:21:58.341613Z","shell.execute_reply":"2021-11-10T02:21:58.357432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_agg.iloc[:, [0, 1, 2, 3, 6, 7, 8, 9]].head()","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:21:12.094524Z","iopub.execute_input":"2021-11-10T02:21:12.095052Z","iopub.status.idle":"2021-11-10T02:21:12.115592Z","shell.execute_reply.started":"2021-11-10T02:21:12.095015Z","shell.execute_reply":"2021-11-10T02:21:12.113942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = ind_agg.corr()\n\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_agg = ind_agg.drop(columns = to_drop)\nind_feats = list(ind_agg.columns)\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint('Final features shape: ', final.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"corrs = final.corr()['Target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs.sort_values().head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrs.sort_values().dropna().tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.groupby('female-head')['Target'].value_counts(normalize=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final.groupby('female-head')['meaneduc'].agg(['mean', 'count'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\nsubmission_base = test[['Id', 'idhogar']].copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = list(train_set.columns)\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), ('scaler', MinMaxScaler())])\n\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100, random_state=10, n_jobs = -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_set, train_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\nfeature_importances.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importances(df, n = 10, threshold = None):\n    \n    plt.style.use('fivethirtyeight')\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    \n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        \n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        \n        \n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"norm_fi = plot_feature_importances(feature_importances, threshold=0.95)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kde_target(df, variable):\n    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n\n    plt.figure(figsize = (12, 8))\n    \n    df = df[df['Target'].notnull()]\n    \n    for level in df['Target'].unique():\n        subset = df[df['Target'] == level].copy()\n        sns.kdeplot(subset[variable].dropna(), \n                    label = f'Poverty Level: {level}', \n                    color = colors[int(subset['Target'].unique())])\n\n    plt.xlabel(variable); plt.ylabel('Density');\n    plt.title('{} Distribution'.format(variable.capitalize()));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kde_target(final, 'meaneduc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kde_target(final, 'escolari/age-range_')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\n\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, LinearSVC(), \n                         'LSVC', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         GaussianNB(), 'GNB', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         MLPClassifier(hidden_layer_sizes=(32, 64, 128, 64, 32)),\n                         'MLP', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, \n                         RidgeClassifierCV(), 'RIDGE', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_set, train_labels, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_set, train_labels, RandomForestClassifier(100, random_state=10), 'RF', model_results)","metadata":{"execution":{"iopub.status.busy":"2021-11-10T02:45:18.770084Z","iopub.execute_input":"2021-11-10T02:45:18.770926Z","iopub.status.idle":"2021-11-10T02:45:18.789438Z","shell.execute_reply.started":"2021-11-10T02:45:18.770874Z","shell.execute_reply":"2021-11-10T02:45:18.788563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                  edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def submit(model, train, train_labels, test, test_ids):\n    model.fit(train, train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar': test_ids,\n                               'Target': predictions})\n\n    submission = submission_base.merge(predictions, \n                                       on = 'idhogar',\n                                       how = 'left').drop(columns = ['idhogar'])\n    \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n\n    return submission ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_submission = submit(RandomForestClassifier(n_estimators = 100, random_state=10, n_jobs = -1), train_set, train_labels, test_set, test_ids)\n\nrf_submission.to_csv('rf_submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = pd.DataFrame(train_set, columns = features)\ncorr_matrix = train_set.corr()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = train_set.drop(columns = to_drop)\ntrain_set.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set = pd.DataFrame(test_set, columns = features)\ntrain_set, test_set = train_set.align(test_set, axis = 1, join = 'inner')\nfeatures = list(train_set.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nestimator = RandomForestClassifier(random_state = 10, n_estimators = 100,  n_jobs = -1)\nselector = RFECV(estimator, step = 1, cv = 3, scoring= scorer, n_jobs = -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selector.fit(train_set, train_labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(selector.grid_scores_);\n\nplt.xlabel('Number of Features'); plt.ylabel('Macro F1 Score'); plt.title('Feature Selection Scores');\nselector.n_features_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rankings = pd.DataFrame({'feature': list(train_set.columns), 'rank': list(selector.ranking_)}).sort_values('rank')\nrankings.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_selected = selector.transform(train_set)\ntest_selected = selector.transform(test_set)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = train_set.columns[np.where(selector.ranking_==1)]\ntrain_selected = pd.DataFrame(train_selected, columns = selected_features)\ntest_selected = pd.DataFrame(test_selected, columns = selected_features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = cv_model(train_selected, train_labels, model, 'RF-SEL', model_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),yerr = list(model_results['cv_std']),edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def macro_f1_score(labels, predictions):\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    return 'macro_f1', metric_value, True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None):\n    \n    \n    feature_names = list(features.columns)\n\n    if hyp is not None:\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        params = {'boosting_type': 'dart', \n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, 'reg_alpha': 0.76, \n                   'reg_lambda': 0.43, \n                   'subsample_for_bin': 40000, \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    \n\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        \n        fold_predictions = pd.DataFrame()\n        \n        \n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        \n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        \n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        \n        fold_probabilitites = model.predict_proba(test_features)\n        \n        \n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n       \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n       \n        predictions = predictions.append(fold_predictions)\n        importances += model.feature_importances_ / nfolds   \n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    \n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n   \n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    \n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    \n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    \n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    return submission, feature_importances, valid_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-display\npredictions, gbm_fi = model_gbm(train_set, train_labels, test_set, test_ids, return_preds=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['font.size'] = 18\n\n\ng = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', size = 3, aspect = 4)\ng.map(sns.kdeplot, 'confidence');\ng.add_legend();\n\nplt.suptitle('Distribution of Confidence by Fold and Target', y = 1.05);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (24, 12))\nsns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predictions.groupby('idhogar', as_index = False).mean()\npredictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\npredictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\npredictions = predictions.drop(columns = ['fold'])\n\n\nplt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'confidence', data = predictions);\nplt.title('Confidence by Target');\n\nplt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'confidence', data = predictions);\nplt.title('Confidence by Target');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, return_preds=False)\n\nsubmission.to_csv('gbm_baseline.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-display\nsubmission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, \n                                                               test_selected, test_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM\", \"GBM_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                                sort = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, \n                                             nfolds=10, return_preds=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('gbm_10fold.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, test_selected, test_ids,\n                                                               nfolds=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\nfrom hyperopt.pyll.stochastic import sample\nimport csv\nimport ast\nfrom timeit import default_timer as timer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(hyperparameters, nfolds=5):\n    \n    \n    \n    global ITERATION\n    ITERATION += 1\n    \n    \n    subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n    subsample_freq = hyperparameters['boosting_type'].get('subsample_freq', 0)\n    \n    boosting_type = hyperparameters['boosting_type']['boosting_type']\n    \n    if boosting_type == 'dart':\n        hyperparameters['drop_rate'] = hyperparameters['boosting_type']['drop_rate']\n    \n    \n    hyperparameters['subsample'] = subsample\n    hyperparameters['subsample_freq'] = subsample_freq\n    hyperparameters['boosting_type'] = boosting_type\n    \n    \n    if not hyperparameters['limit_max_depth']:\n        hyperparameters['max_depth'] = -1\n    \n    \n    for parameter_name in ['max_depth', 'num_leaves', 'subsample_for_bin', \n                           'min_child_samples', 'subsample_freq']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n\n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    \n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    \n    \n    features = np.array(train_selected)\n    labels = np.array(train_labels).reshape((-1 ))\n    \n    valid_scores = []\n    best_estimators = []\n    run_times = []\n    \n    model = lgb.LGBMClassifier(**hyperparameters, class_weight = 'balanced',\n                               n_jobs=-1, metric = 'None',\n                               n_estimators=10000)\n    \n    \n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        \n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        start = timer()\n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score, \n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 400)\n        end = timer()\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        best_estimators.append(model.best_iteration_)\n        \n        run_times.append(end - start)\n    \n    score = np.mean(valid_scores)\n    score_std = np.std(valid_scores)\n    loss = 1 - score\n    \n    run_time = np.mean(run_times)\n    run_time_std = np.std(run_times)\n    \n    estimators = int(np.mean(best_estimators))\n    hyperparameters['n_estimators'] = estimators\n    \n    \n    of_connection = open(OUT_FILE, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, hyperparameters, ITERATION, run_time, score, score_std])\n    of_connection.close()\n    \n    \n    if ITERATION % PROGRESS == 0:\n        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n    \n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n            'score': score, 'score_std': score_std}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space = {\n    'boosting_type': hp.choice('boosting_type', \n                              [{'boosting_type': 'gbdt', \n                                'subsample': hp.uniform('gdbt_subsample', 0.5, 1),\n                                'subsample_freq': hp.quniform('gbdt_subsample_freq', 1, 10, 1)}, \n                               {'boosting_type': 'dart', \n                                 'subsample': hp.uniform('dart_subsample', 0.5, 1),\n                                 'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1),\n                                 'drop_rate': hp.uniform('dart_drop_rate', 0.1, 0.5)},\n                                {'boosting_type': 'goss',\n                                 'subsample': 1.0,\n                                 'subsample_freq': 0}]),\n    'limit_max_depth': hp.choice('limit_max_depth', [True, False]),\n    'max_depth': hp.quniform('max_depth', 1, 40, 1),\n    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),\n    'learning_rate': hp.loguniform('learning_rate', \n                                   np.log(0.025), \n                                   np.log(0.25)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),\n    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample(space)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"algo = tpe.suggest","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trials = Trials()\n\n\nOUT_FILE = 'optimization.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nMAX_EVALS = 100\nPROGRESS = 10\nN_FOLDS = 5\nITERATION = 0\n\n\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']\nwriter.writerow(headers)\nof_connection.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture --no-display\ndisplay(\"Running Optimization for {} Trials.\".format(MAX_EVALS))\n\n\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('trials.json', 'w') as f:\n    f.write(json.dumps(str(trials)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()\nresults.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.regplot('iteration', 'score', data = results);\nplt.title(\"Optimization Scores\");\nplt.xticks(list(range(1, results['iteration'].max() + 1, 3)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_hyp = ast.literal_eval(results.loc[0, 'hyperparameters'])\nbest_hyp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, \n                                             test_selected, test_ids, \n                                             nfolds = 10, return_preds=False)\n\nmodel_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True).sort_values('cv_mean', ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, \n                                             nfolds = 10, return_preds=False)\n\nmodel_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold\"], \n                                                   'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True).sort_values('cv_mean', ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('gbm_opt_10fold_selected.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = submission_base.merge(submission, on = 'Id', how = 'left')\npreds = pd.DataFrame(preds.groupby('idhogar')['Target'].mean())\n\n\nfig, axes = plt.subplots(1, 2, sharey = True, figsize = (12, 6))\nheads['Target'].sort_index().plot.hist(normed = True,\n                                       edgecolor = r'k',\n                                       linewidth = 2,\n                                       ax = axes[0])\n\naxes[0].set_xticks([1, 2, 3, 4]);\naxes[0].set_xticklabels(poverty_mapping.values(), rotation = 60)\naxes[0].set_title('Train Label Distribution')\n\n\npreds['Target'].sort_index().plot.hist(normed = True, \n                                       edgecolor = 'k',\n                                       linewidth = 2,\n                                       ax = axes[1])\naxes[1].set_xticks([1, 2, 3, 4]);\naxes[1].set_xticklabels(poverty_mapping.values(), rotation = 60)\nplt.subplots_adjust()\nplt.title('Predicted Label Distribution');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heads['Target'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds['Target'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_selected, train_labels,test_size = 1000,random_state = 10)\n\n\nmodel = lgb.LGBMClassifier(**best_hyp, class_weight = 'balanced', random_state = 10)\nmodel.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_preds = model.predict_proba(X_valid)\npreds_df = pd.DataFrame(valid_preds, columns = [1, 2, 3, 4])\n\n\npreds_df['prediction'] = preds_df[[1, 2, 3, 4]].idxmax(axis = 1)\npreds_df['confidence'] = preds_df[[1, 2, 3, 4]].max(axis = 1)\n\npreds_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('F1 score:', round(f1_score(y_valid, preds_df['prediction'], average = 'macro'), 5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Oranges):\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.figure(figsize = (10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, size = 24)\n    plt.colorbar(aspect=4)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n    plt.yticks(tick_marks, classes, size = 14)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    \n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.grid(None)\n    plt.tight_layout()\n    plt.ylabel('True label', size = 18)\n    plt.xlabel('Predicted label', size = 18)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_valid, preds_df['prediction'])\n\nplot_confusion_matrix(cm, classes = ['Extreme', 'Moderate', 'Vulnerable', 'Non-Vulnerable'],\n                      title = 'Poverty Confusion Matrix')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(cm, normalize = True,\n                      classes = ['Extreme', 'Moderate', 'Vulnerable', 'Non-Vulnerable'],\n                      title = 'Poverty Confusion Matrix')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from umap import UMAP\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.manifold import TSNE\n\nn_components = 3\n\numap = UMAP(n_components=n_components)\npca = PCA(n_components=n_components)\nica = FastICA(n_components=n_components)\ntsne = TSNE(n_components=n_components","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_selected.copy()\ntest_df = test_selected.copy()\n\nfor method, name in zip([umap, pca, ica, tsne], \n                        ['umap', 'pca', 'ica', 'tsne']):\n    \n    \n    if name == 'tsne':\n        start = timer()\n        reduction = method.fit_transform(train_selected)\n        end = timer()\n    \n    else:\n        start = timer()\n        reduction = method.fit_transform(train_selected)\n        end = timer()\n        \n        test_reduction = method.transform(test_selected)\n        test_df['%s_c1' % name] = test_reduction[:, 0]\n        test_df['%s_c2' % name] = test_reduction[:, 1]\n        test_df['%s_c3' % name] = test_reduction[:, 2]\n\n   \n    train_df['%s_c1' % name] = reduction[:, 0]\n    train_df['%s_c2' % name] = reduction[:, 1]\n    train_df['%s_c3' % name] = reduction[:, 2]\n    \n    print(f'Method: {name} {round(end - start, 2)} seconds elapsed.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\ndef discrete_cmap(N, base_cmap=None):\n\n    base = plt.cm.get_cmap(base_cmap)\n    color_list = base(np.linspace(0, 1, N))\n    cmap_name = base.name + str(N)\n    return base.from_list(cmap_name, color_list, N)\n\ncmap = discrete_cmap(4, base_cmap = plt.cm.RdYlBu)\n\ntrain_df['label'] = train_labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for method, name in zip([umap, pca, ica, tsne], \n                        ['umap', 'pca', 'ica', 'tsne']):\n    \n    fig = plt.figure(figsize = (8, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    p = ax.scatter(train_df['%s_c1' % name], train_df['%s_c2'  % name], train_df['%s_c3'  % name], \n                   c = train_df['label'].astype(int), cmap = cmap)\n    \n    plt.title(f'{name.capitalize()}', size = 22)\n    fig.colorbar(p, aspect = 4, ticks = [1, 2, 3, 4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_df.align(test_df, axis = 1, join = 'inner')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n\nsubmission, gbm_fi, valid_scores = model_gbm(train_df, train_labels, \n                                             test_df, test_ids, nfolds = 10,\n                                             hyp = best_hyp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('gbm_opt_10fold_dr.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold_DR\"], 'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_results = model_results.sort_values('cv_mean')\nmodel_results.set_index('model', inplace = True)\nmodel_results['cv_mean'].plot.bar(color = 'orange', figsize = (10, 8),\n                                  edgecolor = 'k', linewidth = 2,\n                                  yerr = list(model_results['cv_std']))\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = plot_feature_importances(gbm_fi)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(max_depth = 3, n_estimators=10)\nmodel.fit(train_selected, train_labels)\nestimator_limited = model.estimators_[5]\nestimator_limited","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\n\nexport_graphviz(estimator_limited, out_file='tree_limited.dot', feature_names = train_selected.columns,\n                class_names = ['extreme', 'moderate' , 'vulnerable', 'non-vulnerable'],\n                rounded = True, proportion = False, precision = 2, filled = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!dot -Tpng tree_limited.dot -o tree_limited.png","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename = 'tree_limited.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier(max_depth = None, n_estimators=10)\nmodel.fit(train_selected, train_labels)\nestimator_nonlimited = model.estimators_[5]\n\nexport_graphviz(estimator_nonlimited, out_file='tree_nonlimited.dot', feature_names = train_selected.columns,\n                class_names = ['extreme', 'moderate' , 'vulnerable', 'non-vulnerable'],\n                rounded = True, proportion = False, precision = 2)\n\n!dot -Tpng tree_nonlimited.dot -o tree_nonlimited.png -Gdpi=600","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image(filename = 'tree_nonlimited.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}